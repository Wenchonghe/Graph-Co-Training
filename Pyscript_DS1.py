# -*- coding: utf-8 -*-
"""dirWholeTestgh-HR-Collapse-D1-5-training samples-ForSubmission.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ipz2gjJfvEu2y6w5Tf20ZNn_JKFjb9Sq
"""

import trainpdir as tpml #add import fromlib
import trainq as tq   #add import from lib
import sklearn
import scipy.sparse
import numpy as np
import json
import graph   #add import from lib
import random
import os, time, collections, shutil
import scipy.sparse as sp
from scipy import sparse
from sklearn import preprocessing
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
import pandas as pd
from numpy import genfromtxt
import sys
import warnings
from keras.optimizers import Adam, SGD
from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
import matplotlib.pyplot as plt
np.random.seed(1337)
from keras.layers import UpSampling2D     #If you encounter error, change to  :  from tensorflow.keras.layer import UpSampling2D
from keras.layers.normalization import BatchNormalization  #If you encounter error, change to  :  from tensorflow.keras.layer import BatchNormalization, the same for following import. 
from keras.layers.core import SpatialDropout2D, Activation
from keras.models import Model, load_model
from keras.layers import Input
from keras.layers.core import Dropout, Lambda
from keras.layers.convolutional import Conv2D, Conv2DTranspose
from keras.layers.pooling import MaxPooling2D
from keras.layers.merge import concatenate
from keras.callbacks import EarlyStopping, ModelCheckpoint
from keras import backend as K
from keras.utils.np_utils import to_categorical   
from keras.models import load_model
from sklearn.metrics import classification_report
import tensorflow as tf

tf.compat.v1.enable_eager_execution()
tf.executing_eagerly()

TrainLength = 171 #416#171 #
TrainCount = 171
ValCount = 120
TestCount =132 #208  #132 #
row= 224
col = 224
ValCountGraph = 1
valrow = 2120
valcol = 2555
test1row = 2685 #3485 #2685 #
test1col =2460  #2875 #2460 #
CT= 'CT_DS_Factor4/'
Data= 'ChunkData/'
notConvergence = 1
threshold = 0.001
ave_accuracy_old = 0

TrainID = np.array(range(0,TrainLength))
random.Random(2).shuffle(TrainID)
TrainID = TrainID[0:TrainCount]

root_path =" " #add your data root path here
train_folder_path = root_path + "Train/Dataset1/"
validation_folder_path = root_path + "Validation/Dataset1/"
test_folder_path = root_path + "Test/Dataset1/"

#GNN Parameter
params = dict()
params['num_epochs']     =150 #args.num_epochs
params['batch_size']     = 1 #args.batch_size
params['filter']         ='chebyshev5' #args.filter
params['brelu']          = 'b1relu'

params['F_0']            = 2 # Number of graph input features.
params['F_1']            = 2  # Number of graph output features.
params['M_0']            = 1000 # Number of graph nodes.  

params['classnum']            = 2 # Number of output class.  
params['F']              =32 # Number of graph convolutional filters.

params['K']              = 2

params['regularization'] = 1e-3
params['dropout']        = 1
params['learning_rate']  = 1e-4
params['decay_rate']     = 0.99
params['momentum']       = 0.9

params['eval_frequency'] = 70
params['dir_name'] = 'C:/tflow2/Scripts/wenchong/NIPS2020/'

def laplacian(W):
    d = W.sum(axis = 1)
    d[d==0]=1
    d = 1 / d
    D = sp.diags(d.A.squeeze(), 0)
    L = D*W
    return L 

def findclass(predicted):
    newpre = -1*np.zeros((predicted.shape[0],predicted.shape[1]))
    for i in range(predicted.shape[0]):
        for j in range(predicted.shape[1]):
            if predicted[i,j,0] > predicted[i, j, 1]:
                newpre[i,j] = 0
            else:
                newpre[i,j] = 1
    return newpre

def load_data():
    
    
    train_features = []
    #train_features_temp = []
    train_labels = []
    train_features_graph = []
    train_labels_graph = []
    L = []
    L1=[]
    L2=[]
    DS = []
    US = []
    
    for i in range(TrainCount): 
        feature = np.load(train_folder_path+'ChunkData/'+str(i)+"trainFeaturesWithElv.npy")
        feature.shape = row,col,4
        label = np.load(train_folder_path+'ChunkData/'+str(i)+"trainLabel.npy")
        label.shape = row,col,1
        train_features.append(feature)
        train_labels.append(label)
        
        graphfeature = np.load(train_folder_path+'ChunkData/'+str(i)+"trainFeaturesWithElv.npy")
        graphfeature.shape = row,col,4
        graphfeature.shape = row*col,4
        train_features_graph.append(graphfeature)
        
        label = np.reshape(label, (row*col, 1))
        label = to_categorical(label, num_classes = 2)
        train_labels_graph.append(label)

        sm_row = []
        sm_col = []
        sm_data = []
        file= open(train_folder_path+'CT_DS_Factor4_Collapse/'+str(i)+'traincollapseList1.txt', "r")
        for line in file:
            linelist = line.split(",")
            collapselist = [int(ll) for ll in linelist] 
            nodeId = collapselist.pop(0)
            for c in collapselist:
                sm_row.append(nodeId)
                sm_col.append(c)
                sm_data.append(1)
        file.close()
        Dmat = (sparse.csr_matrix((sm_data,(sm_row,sm_col))))
        Umat = Dmat.transpose()
        Dmat = preprocessing.normalize(Dmat,norm='l1',axis=1)
        DS.append(Dmat)
        US.append(Umat)
        sm_row = []
        sm_col = []
        sm_data = []
        file= open(train_folder_path+'CT_DS_Factor4_Collapse/'+str(i)+'traincollapseTree1'+'.txt', "r")
        lineCounter = 0
        for line in file:
            lineCounter +=1
            linelist = line.split(",")
            childlist = [int(i) for i in linelist] 
            nodeId = childlist.pop(0)
            for c in childlist:
                sm_row.append(nodeId)
                sm_col.append(c)
                sm_data.append(1)
        file.close()
        adj = sparse.csr_matrix((sm_data,(sm_row,sm_col)), shape=(lineCounter, lineCounter))
        adj2 = sparse.csr_matrix.transpose(adj)
        L1.append(adj)
        L2.append(adj2)
    
    
    for i in range(0,ValCount):
        feature = np.load(validation_folder_path+'ChunkData/'+str(i)+"validationFeaturesWithElv.npy")
        feature.shape = row,col,4
        label = np.load(validation_folder_path+'ChunkData/'+str(i)+"validationLabel.npy")

        label.shape = row,col,1
        train_features.append(feature)
        train_labels.append(label)
        
    for i in range(0, ValCountGraph):
        feature = np.load(validation_folder_path+'ImageData/'+str(i)+"validationFeaturesWithElv.npy")
        feature.shape = valrow,valcol,4
        feature.shape = valrow*valcol,4
        train_features_graph.append(feature)
        
        label = np.load(validation_folder_path+'ImageData/'+str(i)+"validationLabel.npy")
        label.shape = valrow,valcol,1
        label = np.reshape(label, (valrow*valcol, 1))
        label = to_categorical(label, num_classes = 2)
        train_labels_graph.append(label)
        
        sm_row = []
        sm_col = []
        sm_data = []
        file= open(validation_folder_path+'CT_DS_Factor4_whole_Collapse/'+str(i)+'validationcollapseList1.txt', "r")
        for line in file:
            linelist = line.split(",")
            collapselist = [int(ll) for ll in linelist] 
            nodeId = collapselist.pop(0)
            for c in collapselist:
                sm_row.append(nodeId)
                sm_col.append(c)
                sm_data.append(1)
        file.close()
        Dmat = (sparse.csr_matrix((sm_data,(sm_row,sm_col))))
        Umat = Dmat.transpose()
        Dmat = preprocessing.normalize(Dmat,norm='l1',axis=1)

        DS.append(Dmat)
        US.append(Umat)
    
        sm_row = []
        sm_col = []
        sm_data = []
        file= open(validation_folder_path+'CT_DS_Factor4_whole_Collapse/'+str(i)+'validationcollapseTree1'+'.txt', "r")
        lineCounter = 0
        for line in file:
            lineCounter +=1
            linelist = line.split(",")
            childlist = [int(i) for i in linelist] 
            nodeId = childlist.pop(0)
            for c in childlist:
                sm_row.append(nodeId)
                sm_col.append(c)
                sm_data.append(1)
        file.close()
        adj = sparse.csr_matrix((sm_data,(sm_row,sm_col)), shape=(lineCounter, lineCounter))
        adj2 = sparse.csr_matrix.transpose(adj)
        L1.append(adj)
        L2.append(adj2)        
   
    for i in range(0,TestCount):
        feature = np.load(test_folder_path+'ChunkData/'+str(i)+"testFeaturesWithElv.npy")
        feature.shape = row,col,4
        label = np.load(test_folder_path+'ChunkData/'+str(i)+"testLabel.npy")
        label.shape = row,col,1
        train_features.append(feature)
        train_labels.append(label)
        
    for i in range(0,ValCountGraph):
        feature = np.load(test_folder_path+'ImageData/'+str(i)+"testFeaturesWithElv.npy")
        feature.shape = test1row,test1col,4
        feature.shape = test1row*test1col,4
        train_features_graph.append(feature)
        
        label = np.load(test_folder_path+'ImageData/'+str(i)+"testLabel.npy")
        label.shape = test1row,test1col,1
        label = np.reshape(label, (test1row*test1col, 1))
        label = to_categorical(label, num_classes = 2)
        train_labels_graph.append(label)
        
        
        sm_row = []
        sm_col = []
        sm_data = []
        file= open(test_folder_path+'CT_DS_Factor4_whole_Collapse/'+str(i)+'testcollapseList1.txt', "r")
        for line in file:
            linelist = line.split(",")
            collapselist = [int(ll) for ll in linelist] 
            nodeId = collapselist.pop(0)
            for c in collapselist:
                sm_row.append(nodeId)
                sm_col.append(c)
                sm_data.append(1)
        file.close()
        Dmat = (sparse.csr_matrix((sm_data,(sm_row,sm_col))))
        Umat = Dmat.transpose()
        Dmat = preprocessing.normalize(Dmat,norm='l1',axis=1)
        DS.append(Dmat)
        US.append(Umat)
        
        sm_row = []
        sm_col = []
        sm_data = []
        file= open(test_folder_path+'CT_DS_Factor4_whole_Collapse/'+str(i)+'testcollapseTree1'+'.txt', "r")
        lineCounter = 0
        for line in file:
            lineCounter +=1
            linelist = line.split(",")
            childlist = [int(i) for i in linelist] 
            nodeId = childlist.pop(0)
            for c in childlist:
                sm_row.append(nodeId)
                sm_col.append(c)
                sm_data.append(1)
        file.close()
        adj = sparse.csr_matrix((sm_data,(sm_row,sm_col)), shape=(lineCounter, lineCounter))
        adj2 = sparse.csr_matrix.transpose(adj)
        L1.append(adj)
        L2.append(adj2)
        
    
    L1 = map(lambda x:x.astype('float32'), L1)
    L1 = [laplacian(a) for a in L1]
    
    L2 = map(lambda x:x.astype('float32'), L2)
    L2 = [laplacian(a) for a in L2]

    train_features = np.array(train_features)
    train_features_graph = np.array(train_features_graph)
    train_labels = np.array(train_labels)
    train_labels_graph = np.array(train_labels_graph)
    val_idx = [i for i in range (TrainCount, TrainCount + ValCount)]
    test_idx = [i for i in range (TrainCount + ValCount, TrainCount + ValCount + TestCount)]
    val_idx = np.array(val_idx)
    test_idx = np.array(test_idx)
    return train_features, train_features_graph, train_labels, train_labels_graph, val_idx, test_idx,L1,L2, DS, US

def dice_coef(y_true, y_pred):
    y_true_f = K.flatten(y_true)
    y_pred_f = K.flatten(y_pred)
    intersection = K.sum(y_true_f * y_pred_f)
    return (2.0 * intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) + 1.0)


def jacard_coef(y_true, y_pred):
    y_true_f = K.flatten(y_true)
    y_pred_f = K.flatten(y_pred)
    intersection = K.sum(y_true_f * y_pred_f)
    return (intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + 1.0)


def jacard_coef_loss(y_true, y_pred):
    return -jacard_coef(y_true, y_pred)


def dice_coef_loss(y_true, y_pred):
    return -dice_coef(y_true, y_pred)

def double_conv_layer(x, size, dropout=0.0, batch_norm=True):
    if K.image_data_format() == 'th':
        axis = 1
    else:
        axis = 3
    conv = Conv2D(size, (3, 3), padding='same')(x)
    if batch_norm is True:
        conv = BatchNormalization(axis=axis)(conv)
    conv = Activation('relu')(conv)
    conv = Conv2D(size, (3, 3), padding='same')(conv)
    if batch_norm is True:
        conv = BatchNormalization(axis=axis)(conv)
    conv = Activation('relu')(conv)
    if dropout > 0:
        conv = SpatialDropout2D(dropout)(conv)
    return conv

def plot_history(history,location):
    loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' not in s]
    val_loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' in s]
    acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' not in s]
    val_acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' in s]
    dice_list = [s for s in history.history.keys() if 'dice' in s and 'val' not in s]
    val_dice_list = [s for s in history.history.keys() if 'dice' in s and 'val' in s]
    
    if len(loss_list) == 0:
        print('Loss is missing in history')
        return 
    epochs = range(1,len(history.history[loss_list[0]]) + 1)
    
    plt.figure(1)
    for l in loss_list:
        plt.plot(epochs, history.history[l], 'b', label='Training loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))
    for l in val_loss_list:
        plt.plot(epochs, history.history[l], 'g', label='Validation loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))
    
    plt.title('Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    
    ## Dice Coefficient
    plt.figure(2)
    for l in dice_list:
        plt.plot(epochs, history.history[l], 'b', label='Training Dice Coefficient (' + str(format(history.history[l][-1],'.5f'))+')')
    for l in val_dice_list:    
        plt.plot(epochs, history.history[l], 'g', label='Validation Dice Coefficient (' + str(format(history.history[l][-1],'.5f'))+')')

    plt.title('Dice Coefficient')
    plt.xlabel('Epochs')
    plt.ylabel('Dice Coefficient')
    plt.legend()
    
    ## Accuracy
    fig=plt.figure(3)
    for l in acc_list:
        plt.plot(epochs, history.history[l], 'b', label='Training accuracy (' + str(format(history.history[l][-1],'.5f'))+')')
    for l in val_acc_list:    
        plt.plot(epochs, history.history[l], 'g', label='Validation accuracy (' + str(format(history.history[l][-1],'.5f'))+')')

    plt.title('Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.show()
    
    fig.savefig(location, dpi=fig.dpi)


IMG_WIDTH = 224
IMG_HEIGHT = 224

# Number of image channels (for example 3 in case of RGB, or 1 for grayscale images)
INPUT_CHANNELS = 4
# Number of output masks (1 in case you predict only one type of objects)
OUTPUT_MASK_CHANNELS = 2

def UNET(dropout_val=0.2, weights=None):
    if K.image_data_format() == 'th':
        inputs = Input((INPUT_CHANNELS, 224, 224))
        axis = 1
    else:
        inputs = Input((224, 224, INPUT_CHANNELS))
        axis = 3
    filters = 32

    conv_224 = double_conv_layer(inputs, filters)
    pool_112 = MaxPooling2D(pool_size=(2, 2))(conv_224)

    conv_112 = double_conv_layer(pool_112, 2*filters)
    pool_56 = MaxPooling2D(pool_size=(2, 2))(conv_112)

    conv_56 = double_conv_layer(pool_56, 4*filters)
    pool_28 = MaxPooling2D(pool_size=(2, 2))(conv_56)

    conv_28 = double_conv_layer(pool_28, 8*filters)
    pool_14 = MaxPooling2D(pool_size=(2, 2))(conv_28)

    conv_14 = double_conv_layer(pool_14, 16*filters)
    pool_7 = MaxPooling2D(pool_size=(2, 2))(conv_14)

    conv_7 = double_conv_layer(pool_7, 32*filters)

    up_14 = concatenate([UpSampling2D(size=(2, 2))(conv_7), conv_14], axis=axis)
    up_conv_14 = double_conv_layer(up_14, 16*filters)

    up_28 = concatenate([UpSampling2D(size=(2, 2))(up_conv_14), conv_28], axis=axis)
    up_conv_28 = double_conv_layer(up_28, 8*filters)

    up_56 = concatenate([UpSampling2D(size=(2, 2))(up_conv_28), conv_56], axis=axis)
    up_conv_56 = double_conv_layer(up_56, 4*filters)

    up_112 = concatenate([UpSampling2D(size=(2, 2))(up_conv_56), conv_112], axis=axis)
    up_conv_112 = double_conv_layer(up_112, 2*filters)

    up_224 = concatenate([UpSampling2D(size=(2, 2))(up_conv_112), conv_224], axis=axis)
    up_conv_224 = double_conv_layer(up_224, filters, dropout_val)

    conv_final = Conv2D(OUTPUT_MASK_CHANNELS, (1, 1))(up_conv_224)
    conv_final = Activation('softmax')(conv_final)

    model = Model(inputs, conv_final, name="ZF_UNET_224")
    return model

def DataGenerator(image,filecounter = 0, filetype="train",
                     stride=80,rlength=300, clength=300,cstart= 0,rstart = 0, bottomUp= False,rightLeft=False,bottomRightCorner=False):
    rend = rstart+rlength
    cend = cstart+clength
    newimg=[]
    #top to bottom
    while True:
        temp_img = np.zeros((rlength,clength,image.shape[2]))
        temp_img[0:rlength,0:clength] = image[rstart:rend,cstart:cend]

        print(filecounter, end= " ")
    
        newimg.append(temp_img)
        
        filecounter +=1
    
        cstart = cstart+stride
        cend = cstart+clength
        if cend>image.shape[1]:
            cstart =0
            cend= clength
            rstart= rstart+stride
            rend = rstart+rlength
            if rend>image.shape[0]:
                break
                
    #bottom to up
    if bottomUp:
        rend = image.shape[0]-1
        rstart = rend -rlength
        cstart = 0
        cend = clength
        while True:
            temp_img = np.zeros((rlength,clength,image.shape[2]))
            temp_img[0:rlength,0:clength] = image[rstart:rend,cstart:cend]
        
            #print(filecounter,end=" ")
            newimg.append(temp_img)
            
            filecounter +=1
        
            cstart = cstart+stride
            cend = cstart+clength
            if cend>image.shape[1]:
                break  
    # right to left
    if rightLeft:
        cend = image.shape[1]
        cstart = cend-clength
        rstart= 0
        rend = rstart+rlength
        while True:
            temp_img = np.zeros((rlength,clength,image.shape[2]))
            temp_img[0:rlength,0:clength] = image[rstart:rend,cstart:cend]
           
            #print(filecounter,end=" ")
            newimg.append(temp_img)
            
            filecounter +=1
            rstart=rstart+stride
            rend =rstart +rlength
            if rend>image.shape[0]:
                break
    if bottomRightCorner:
        cend = image.shape[1]
        cstart = cend-clength
        rstart= image.shape[0]-rlength
        rend = image.shape[0]
        temp_img = np.zeros((rlength,clength,image.shape[2]))
        temp_img[0:rlength,0:clength] = image[rstart:rend,cstart:cend]

        #print(filecounter,end=" ")
        newimg.append(temp_img)
        filecounter +=1
    newimg = np.array(newimg)
    print(filetype+" Data generation Complete")
    return newimg

train_features,train_features_graph, train_labels, train_labels_graph, val_idx, test_idx, L1,L2, DS, US = load_data()
train_labels = to_categorical(train_labels, num_classes = 2)
for i in range(train_features.shape[0]):
    train_features[i,:,:,0:3] = train_features[i,:,:,0:3]/255
    
for i in range(train_features_graph.shape[0]):
    train_features_graph[i][:,0:3] = train_features_graph[i][:, 0:3]/255
train_graph_features = train_features_graph



X_train = train_features[range(TrainCount)]
Y_train = train_labels[range(TrainCount)]
X_validation = train_features[val_idx]
Y_validation =train_labels[val_idx]

warnings.filterwarnings('ignore', category=UserWarning, module='skimage')

modelq = UNET()
learning_rate =0.0001
patience = 30
modelq.compile(optimizer=Adam(lr=learning_rate),loss ='categorical_crossentropy',metrics=['accuracy'])
callbacks = [
        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-9, min_delta=0.00001, verbose=1, mode='min'),
        EarlyStopping(monitor='val_loss', patience=patience, verbose=0),
        ModelCheckpoint('UnetElv.h5', monitor='val_loss', save_best_only=True, verbose=0),
    ]
results_01 = modelq.fit(X_train , Y_train,validation_data=(X_validation,Y_validation), batch_size=5, epochs=400,
                   callbacks=callbacks)

location = "unetTR.pdf"
plot_history(results_01,location)

"""# EM Iteration

"""

iteration = 0
maxIteration = 10
while notConvergence and iteration < maxIteration:
    iteration = iteration + 1
    print(iteration)
    modelq = load_model('UnetElv.h5')
    test_predicted = np.zeros((valrow,valcol,2))
    test_groundtruth = np.zeros((valrow,valcol,2))
    tid = 0
    cstart = 0
    clength = 224
    rstart = 0
    rlength =224
    rend = rstart+rlength
    cend = cstart+clength
    fileid = 0
    stride = 224
    while True:
    #     X_test = np.load(testpath+str(fileid)+"testFeaturesWithElv.npy")
    #     X_test.shape = 1,224,224,4
        test = X_validation[fileid]
        test.shape = 1,224,224,4
        preds_test = modelq.predict(test)
        test_predicted[rstart:rend,cstart:cend] = preds_test
        test_groundtruth[rstart:rend,cstart:cend] = Y_validation[fileid]
        #a = findclass(test_groundtruth[rstart:rend,cstart:cend])
        #plt.imshow(a)

        fileid = fileid + 1
        #print("Firstfileid= ",fileid, end= " ")
        preds_test = np.array(preds_test)
        preds_test.shape = 224,224,2
        test_predicted[rstart:rend,cstart:cend] = preds_test
        #update row and col pointers
        cstart = cstart+stride
        cend = cstart+clength
        if cend>test_predicted.shape[1]:
            cstart =0
            cend= clength
            rstart= rstart+stride
            rend = rstart+rlength
            if rend>test_predicted.shape[0]:
                break

    #bottom to up
    #if bottomUp:
    rend = test_predicted.shape[0]-1
    rstart = rend -rlength
    cstart = 0
    cend = clength
    while True:
        test = X_validation[fileid]
        test.shape = 1,224,224,4
        preds_test = modelq.predict(test)
        test_predicted[rstart:rend,cstart:cend] = preds_test
        test_groundtruth[rstart:rend,cstart:cend] = Y_validation[fileid]
        fileid = fileid + 1
        #print("Secondfileid= ",fileid, end= " ")
        preds_test = np.array(preds_test)
        preds_test.shape = 224,224,2
        test_predicted[rstart:rend,cstart:cend] = preds_test
        #update row and col pointers
        cstart = cstart+stride
        cend = cstart+clength
        if cend>test_predicted.shape[1]:
            break 

    #if rightLeft:
    cend = test_predicted.shape[1]
    cstart = cend-clength
    rstart= 0
    rend = rstart+rlength
    while True:
        test = X_validation[fileid]
        test.shape = 1,224,224,4
        preds_test = modelq.predict(test)
        test_predicted[rstart:rend,cstart:cend] = preds_test
        test_groundtruth[rstart:rend,cstart:cend] = Y_validation[fileid]
        fileid = fileid + 1
        #print("Thirdfileid= ",fileid, end= " ")
        preds_test = np.array(preds_test)
        preds_test.shape = 224,224,2
        test_predicted[rstart:rend,cstart:cend] = preds_test
        rstart=rstart+stride
        rend =rstart +rlength
        if rend>test_predicted.shape[0]:
            break

    #if bottomRightCorner:
    cend = test_predicted.shape[1]
    cstart = cend-clength
    rstart= test_predicted.shape[0]-rlength
    rend = test_predicted.shape[0]
    test = X_validation[fileid]
    test.shape = 1,224,224,4
    preds_test = modelq.predict(test)
    test_predicted[rstart:rend,cstart:cend] = preds_test
    test_groundtruth[rstart:rend,cstart:cend] = Y_validation[fileid]

    fileid = fileid + 1
    #print("Lastfileid= ",fileid, end= " ")
    preds_test = np.array(preds_test)
    preds_test.shape = 224,224,2
    test_predicted[rstart:rend,cstart:cend] = preds_test


    val_predict = test_predicted
    val_gt = test_groundtruth
    gt = findclass(val_gt)
    pr = findclass(val_predict)

    test_predicted = np.zeros((test1row,test1col,2))
    test_groundtruth = np.zeros((test1row,test1col,2))
    X_validation = train_features[np.concatenate((val_idx,test_idx), axis=None)]
    Y_validation =train_labels[np.concatenate((val_idx,test_idx), axis=None)]
    testpath= "C:\\tflow2\\Scripts\\wenchong\\KDD20\\Test\\Data\\"
    tid = 0
    cstart = 0
    clength = 224
    rstart = 0
    rlength =224
    rend = rstart+rlength
    cend = cstart+clength
    fileid = 120
    stride =224
    while True:

        test = X_validation[fileid]
        test.shape = 1,224,224,4
        preds_test = modelq.predict(test)
        test_predicted[rstart:rend,cstart:cend] = preds_test
        test_groundtruth[rstart:rend,cstart:cend] = Y_validation[fileid]


        fileid = fileid + 1

        #print("Firstfileid= ",fileid, end= " ")
        preds_test = np.array(preds_test)
        preds_test.shape = 224,224,2
        test_predicted[rstart:rend,cstart:cend] = preds_test
        #update row and col pointers
        cstart = cstart+stride
        cend = cstart+clength
        if cend>test_predicted.shape[1]:
            cstart =0
            cend= clength
            rstart= rstart+stride
            rend = rstart+rlength
            if rend>test_predicted.shape[0]:
                break

    #bottom to up
    #if bottomUp:
    rend = test_predicted.shape[0]-1
    rstart = rend -rlength
    cstart = 0
    cend = clength
    while True:
        test = X_validation[fileid]
        test.shape = 1,224,224,4
        preds_test = modelq.predict(test)
        test_predicted[rstart:rend,cstart:cend] = preds_test
        test_groundtruth[rstart:rend,cstart:cend] = Y_validation[fileid]

        fileid = fileid + 1
        #print("Secondfileid= ",fileid, end= " ")
        preds_test = np.array(preds_test)
        preds_test.shape = 224,224,2
        test_predicted[rstart:rend,cstart:cend] = preds_test
        #update row and col pointers
        cstart = cstart+stride
        cend = cstart+clength
        if cend>test_predicted.shape[1]:
            break 

    #if rightLeft:
    cend = test_predicted.shape[1]
    cstart = cend-clength
    rstart= 0
    rend = rstart+rlength
    while True:
        test = X_validation[fileid]
        test.shape = 1,224,224,4
        preds_test = modelq.predict(test)
        test_predicted[rstart:rend,cstart:cend] = preds_test
        test_groundtruth[rstart:rend,cstart:cend] = Y_validation[fileid]

        fileid = fileid + 1
        #print("Thirdfileid= ",fileid, end= " ")
        preds_test = np.array(preds_test)
        preds_test.shape = 224,224,2
        test_predicted[rstart:rend,cstart:cend] = preds_test
        rstart=rstart+stride
        rend =rstart +rlength
        if rend>test_predicted.shape[0]:
            break

    #if bottomRightCorner:
    cend = test_predicted.shape[1]
    cstart = cend-clength
    rstart= test_predicted.shape[0]-rlength
    rend = test_predicted.shape[0]
    test = X_validation[fileid]
    test.shape = 1,224,224,4
    preds_test = modelq.predict(test)
    test_predicted[rstart:rend,cstart:cend] = preds_test
    test_groundtruth[rstart:rend,cstart:cend] = Y_validation[fileid]

    fileid = fileid + 1
    #print("Lastfileid= ",fileid, end= " ")
    preds_test = np.array(preds_test)
    preds_test.shape = 224,224,2
    test_predicted[rstart:rend,cstart:cend] = preds_test

    test1_predict = test_predicted
    test1_gt = test_groundtruth
    pr = findclass(test1_predict)
    gt = findclass(test1_gt)
    
    fig = plt.figure()
    plt.imshow(pr)
    loca = 'C:/tflow2/Scripts/wenchong/NIPS2020/prediction/unetPR_'+str(iteration)+'.pdf'
    fig.savefig(loca, dpi=fig.dpi)
    
    y_true2 = np.copy(gt)
    y_true2.shape = y_true2.shape[0]*y_true2.shape[1]
    y_pred2 = np.copy(pr)
    y_pred2.shape = y_pred2.shape[0]*y_pred2.shape[1]
    y_pred2 = np.array(y_pred2,dtype='uint8')
    y_true2 = np.array(y_true2,dtype='uint8')
    confusion_matrix(y_true2, y_pred2, labels=[0, 1])

    report_unet = classification_report(y_true2, y_pred2)

    ave_accuracy_unet = accuracy_score(y_true2, y_pred2)
    print("report_unet", report_unet)
    print("accuracy_unet", ave_accuracy_unet)
    #np.savetxt

    #Train q on graph

    n_train = train_features.shape[0]
    params['decay_steps']    = n_train / params['batch_size']
    modelp = tpml.gnn(**params)

    train_labels_colgraph = []
    for i in range(train_labels_graph.shape[0]):
        temp = np.array(sparse.csr_matrix(DS[i]).dot(sparse.csr_matrix(train_labels_graph[i])).todense())
        temp[temp>=0.5] = 1
        temp[temp<0.5] = 0
        train_labels_colgraph.append(temp)
    train_labels_colgraph = np.array(train_labels_colgraph)

    labels_predict_colgraph = []
    for i in range(TrainCount):
        labels_predict_colgraph.append(train_labels_colgraph[i])

    val_predict.shape = val_predict.shape[0]*val_predict.shape[1], 2
    test1_predict.shape = test1_predict.shape[0]*test1_predict.shape[1], 2

    valtemp = np.array(sparse.csr_matrix(DS[TrainCount]).dot(sparse.csr_matrix(val_predict)).todense())
    valtemp[valtemp>=0.5] = 1
    valtemp[valtemp<0.5] = 0
    labels_predict_colgraph.append(valtemp)


    testtemp = np.array(sparse.csr_matrix(DS[TrainCount+1]).dot(sparse.csr_matrix(test1_predict)).todense())
    testtemp[testtemp>=0.5] = 1
    testtemp[testtemp<0.5] = 0
    labels_predict_colgraph.append(testtemp)

    labels_predict_colgraph =np.array(labels_predict_colgraph)

    true_labels_graph = train_labels_colgraph
    true_labels_graph = np.array(true_labels_graph)
    train_data = labels_predict_colgraph
    train_idx = range(TrainCount)
    valtest=np.array([TrainCount,TrainCount+1])

    predict = modelp.fit(train_data, labels_predict_colgraph,  true_labels_graph, train_idx, valtest,L1,L2)

    predict[0].shape = predict[0].shape[1],predict[0].shape[2]
    predict[1].shape = predict[1].shape[1], predict[1].shape[2]

    valtemp = np.array(sparse.csr_matrix(US[TrainCount]).dot(sparse.csr_matrix(predict[0])).todense())
    valtemp[valtemp>=0.5] = 1
    valtemp[valtemp<0.5] = 0
    valpred =np.reshape(valtemp, (valrow, valcol, 2))

    testtemp = np.array(sparse.csr_matrix(US[TrainCount+1]).dot(sparse.csr_matrix(predict[1])).todense())
    testtemp[testtemp>=0.5] = 1
    testtemp[testtemp<0.5] = 0
    test1pred = np.reshape(testtemp,  (test1row, test1col, 2))

    pred = findclass(test1pred)

    y_true3 = np.copy(gt)
    y_true3.shape = y_true3.shape[0]*y_true3.shape[1]
    y_pred3 = np.copy(pred)
    y_pred3.shape = y_pred3.shape[0]*y_pred3.shape[1]
    y_pred3 = np.array(y_pred3,dtype='uint8')
    y_true3 = np.array(y_true3,dtype='uint8')
    confusion_matrix(y_true3, y_pred3, labels=[0, 1])

    report_gnn = classification_report(y_true3, y_pred3)
    ave_accuracy_gnn = accuracy_score(y_true3, y_pred3)
    print("report_gnn:", report_gnn)
    print("ave_accuracy_gnn", ave_accuracy_gnn)

    if np.absolute(ave_accuracy_gnn - ave_accuracy_old) < threshold:
        notConvergence  = 0   

    ###Annotate val/test region label

    valimgpred = DataGenerator(valpred,filecounter = 0, filetype="validation",
                         stride=224,rlength=224, clength=224,cstart= 0,rstart = 0, bottomUp= True,rightLeft=True,bottomRightCorner=True)
    test1imgpred = DataGenerator(test1pred,filecounter = 0, filetype="test1",
                         stride=224,rlength=224, clength=224,cstart= 0,rstart = 0, bottomUp= True,rightLeft=True,bottomRightCorner=True)
    labels_predicted= np.concatenate((valimgpred,test1imgpred), axis = 0)

    labels_predict_graph_new = labels_predicted
    labels_predict_graph_new = np.concatenate((train_labels[range(TrainCount)], labels_predict_graph_new), axis=0 )

    ### Train U-net again

    X_train = train_features
    Y_train = labels_predict_graph_new
    X_validation = train_features[val_idx]
    Y_validation =train_labels[val_idx]

    modelqnew = UNET()
    learning_rate =0.0001
    patience = 70
    modelqnew.compile(optimizer=Adam(lr=learning_rate),loss ='categorical_crossentropy',metrics=['accuracy'])
    callbacks = [
            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-9, min_delta=0.00001, verbose=1, mode='min'),
            EarlyStopping(monitor='val_loss', patience=patience, verbose=0),
            ModelCheckpoint('UnetElv.h5', monitor='val_loss', save_best_only=True, verbose=0),
        ]

    results = modelqnew.fit(X_train , Y_train,validation_data=(X_validation,Y_validation), batch_size=10, epochs=70,
                       callbacks=callbacks)
    
    location = "unetTR_"+str(iteration)+".pdf"
    plot_history(results,location)

